# Transform-Motion-into-Sound-Effects
By using IMU sensors we collect the movements of a juggling club in real time and interpret them as sound effect signals to an on playing song.

Using an android application called HyperIMU (Inertial Measurement Unit), we managed to gain access to Micro-Electro-Mechanical System (MEMS) sensors straight from our cell phones. HyperIMU allowed us to select the different sensors that exist in smartphones and collect the motion data. The physical sensors used were the accelerometer, gyro, and magnetometer. Through these sensors we implement sensor fusion to take advantage of virtual sensors, such as the linear accelerometer, gravity, rotation vector and compass. Another great feature of the application was the network protocols that were available. We used User Datagram Protocol (UDP) to transmit the sensor data in real time from the phone to a computer, and then read it in Python and Matlab. By activating HyperIMU on our phones and securing the phones in
juggling clubs, we were able to juggle and perform real time plotting in Matlab. The motion data was transmitted every 20ms as a CSV file. This data was used to create threshold conditions, which when satisfied, would give us a desired audio effect. The initial idea was to figure out whether the incoming data from the app was useful or not. We also needed to know if it needed some kind of filtering to make it usable. To achieve this, we decided to plot in real time the incoming data from all the sensors that we had decided to incorporate and observe them while performing various juggling patterns to get different conditions. We developed a code in MATLAB using the UDP library to get the sensor data being transmitted by the android application via UDP. We then plotted all the 3 axis data of each sensor in real-time. Figure 1 shows a real time plot of the data collected from the rotational vector.


As it turned out, all the plots were smooth enough and needed no filtering. Afterwards, we observed all twelve plots and constructed five different conditions based on the behavior of the plots. Each condition was used to implement a different audio effect. The five audio effects we decided to implement were amplitude modulation, vibrato, robotization, reverberation and echo, which were all implemented (in real time) on a wave file. Our wave file was Charlie Puth’s hit song, “We Don’t Talk Anymore,” featuring Selena Gomez. A vibrato is a musical effect that involves a slight variation in pitch to produce a richer tone and a reverb is the persistence of a sound after it has been produced, caused by reflections which build up, only to decay once the sound waves are absorbed by objects in the environment. We asked Selena Gomez for permission and she said it was okay to use her song (we actually didn’t, but for now, let us assume copyright is not an issue...we will eventually address this issue if we take this project any further). 

We knew that we could get good conditions by observing the gyroscope data for the normal juggling throw. In a similar way, we observed different sensor data for different motions until we filtered out five robust conditions. We decided that the amplitude modulation would be a result of the faceup toss, as shown in Figure 2, and the robotization effect would take place when the juggling pins were upright, as shown in Figure 3. The reverberation effect would take place when a circular motion occurred, as shown in Figure 4, and the echo effect would take place whenever the jugglers performed a weave with the pin, as shown in Figure 5. We went with these specific motions because these are the most common types of movements in juggling.

Although real time plotting could be done in Python, we decided to use Matlab because Matlab plots are visually more friendly than Python plots. In our Python code, we decided that the amplitude modulation (duck sound) would take place whenever the Z-Gravity Sensor was greater than 7 and the robotization effect would take place whenever the Y-Gravity Sensor was greater than 7. The vibrato would take place whenever the Z-Gyroscope was above .5 and the reverberation effect (feedback) would take place whenever √(X − Linear acceleration)2 + (Z − Linear Acceleration)2 was greater than 3. The echo effect would take place whenever |X − Rotational V ector| + |Z − rotational vector| was greater than 2 

